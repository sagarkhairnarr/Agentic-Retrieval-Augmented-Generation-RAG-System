# ğŸ“š Agentic Retrieval-Augmented Generation (RAG) System â€” AWS Guidance

This project implements an **Agentic RAG (Retrieval Augmented Generation) System** that answers user questions using information strictly extracted from the **AWS Prescriptive Guidance RAG PDF**.
The system retrieves relevant document chunks and generates grounded responses while preventing hallucination.

---

## ğŸš€ Features

* ğŸ“„ PDF ingestion & preprocessing
* âœ‚ï¸ Semantic chunking into overlapping text blocks
* ğŸ§  Local HuggingFace embeddings (no OpenAI quota needed for embeddings)
* ğŸ” Vector search using ChromaDB
* ğŸ¤– Three-agent architecture:

  * **Planner Agent** â€” Understands query & decides retrieval
  * **Retriever Agent** â€” Searches relevant chunks
  * **Synthesis Agent** â€” Generates grounded answer using LLM
* ğŸ›‘ Zero hallucination enforcement
* ğŸ§ª Terminal-based user interaction for runtime questions

---

## ğŸ—ï¸ Architecture Overview

```
User Question
     â†“
Planner Agent â”€â”€â”€ Determines intent & retrieval plan
     â†“
Retriever Agent â”€â”€â”€ Fetches relevant chunks from vector DB
     â†“
Synthesis Agent â”€â”€â”€ Generates grounded response using LLM
     â†“
Grounded Final Answer
```

---

## ğŸ“‚ Project Structure

```
agentic-rag-aws/
â”‚
â”œâ”€â”€ app.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ aws_rag_guide.pdf
â”‚
â”œâ”€â”€ ingestion/
â”‚   â”œâ”€â”€ load_pdf.py
â”‚   â”œâ”€â”€ chunking.py
â”‚   â””â”€â”€ embed_index.py
â”‚
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ planner_agent.py
â”‚   â”œâ”€â”€ retriever_agent.py
â”‚   â””â”€â”€ synthesis_agent.py
â”‚
â””â”€â”€ graph/
    â””â”€â”€ rag_graph.py
```

---

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Create Environment (Anaconda recommended)

```bash
conda create -n agentic-rag python=3.10
conda activate agentic-rag
```

### 2ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

### 3ï¸âƒ£ Add API Key (required for generation step)

Copy the example file and set your key:

```bash
cp .env.example .env
```

Edit `.env` and paste your OpenAI key:

```
OPENAI_API_KEY=your_api_key_here
```

> ğŸ’¡ Embeddings run locally â€” only synthesis uses OpenAI.

### 4ï¸âƒ£ Run the System

```bash
python app.py
```

The system will:

* load the PDF
* chunk documents
* build vector index
* start interactive question mode

---

## ğŸ“¥ Ingestion Process (PDF â†’ Vector Index)

Manually:

```python
from ingestion.load_pdf import load_pdf
from ingestion.chunking import chunk_documents
from ingestion.embed_index import create_vectorstore

docs = load_pdf("data")
chunks = chunk_documents(docs)
store = create_vectorstore(chunks)
```

Automatically: `python app.py`

---

## ğŸ’¬ Example Queries

```
Compare fully managed RAG and custom RAG
What are the retrieval options mentioned in the guide?
How does RAG differ from fine-tuning?
Is WhatsApp integration mentioned?
```

Expected behavior:

* ğŸ” **If answer exists** â†’ grounded response
* â“ **If not found** â†’ `Information not available in the document.`

---



## ğŸ§‘â€ğŸ’» Author

**Sagar Khairnar**
Data Scientist | Python | Machine Learning | RAG Systems

---

